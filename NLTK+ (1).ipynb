{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\ravi teja\\anaconda3\\lib\\site-packages\n",
      "Requirement already satisfied: six in c:\\users\\ravi teja\\anaconda3\\lib\\site-packages (from nltk)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 9.0.1, however version 9.0.2 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "# pip install\n",
    "\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import and download\n",
    "\n",
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "Chopping off a given document into small pieces is known as tokenization.\n",
    "\n",
    "These small pieces of text are called as tokens. \n",
    "\n",
    "Sentence tokenization chops a document or article into sentences.\n",
    "\n",
    "word tokenization chops a document or article down to words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# importing tokenizers\n",
    "\n",
    "from nltk import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This is python class.', 'I like python.', 'I am a student.', 'This is last line.']\n"
     ]
    }
   ],
   "source": [
    "# using sentence tokenizer\n",
    "\n",
    "example_text = \"This is python class. I like python.  I am a student. This is last line. \"\n",
    "sent = sent_tokenize(example_text)\n",
    "print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'python', 'class', '.', 'I', 'like', 'python', '.', 'I', 'am', 'a', 'student', '.', 'This', 'is', 'last', 'line', '.']\n"
     ]
    }
   ],
   "source": [
    "# word tokenizer\n",
    "\n",
    "words = word_tokenize(example_text)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'python', 'class.', 'I', 'like', 'python.', 'I', 'am', 'a', 'student.', 'This', 'is', 'last', 'line.']\n"
     ]
    }
   ],
   "source": [
    "#using .split() function\n",
    "\n",
    "example_words = example_text.split() \n",
    "print (example_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing\n",
    "\n",
    "Text is the most unstructured form of all the available data, various types of noise are present in it and the data is not readily analyzable without any pre-processing.\n",
    "\n",
    "There are three major ways of text preprocessing. \n",
    "1. Noise reduction\n",
    "2. Lexicon normalization\n",
    "3. Object standarization \n",
    "\n",
    "#### Noise reduction:\n",
    "\n",
    "Any piece of text which is not relevant to the context of the data and the end-output can be specified as the noise.\n",
    "\n",
    "Ex: 'is' , 'or' , 'and' , 'the'. \n",
    "\n",
    "These words can also be called as stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Method 1 :  stopwords removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#importing stopwords from nltk.corpus\n",
    "\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'y', 'to', \"couldn't\", 'very', 'has', 'will', 'out', \"won't\", 'below', \"she's\", \"you've\", 'shouldn', 'through', 'above', 'our', 'again', 'so', 'was', 'here', 'only', 'on', 'o', \"you'd\", \"mightn't\", 'both', 'hers', 'your', \"don't\", 'him', 'll', 'further', \"haven't\", 'such', 'wouldn', 'own', 'himself', 'with', 'as', 'he', 'but', \"weren't\", 'hadn', \"wasn't\", 's', 'what', 'how', 'off', 'no', 'these', 'their', \"it's\", \"doesn't\", 'more', \"isn't\", 'or', 'an', \"shouldn't\", 'having', 'any', 'mustn', 'the', 'herself', 'ma', 'after', 'of', \"hadn't\", \"didn't\", 'theirs', 't', 'itself', 'and', 'ain', 'now', 'it', 'does', 'down', \"that'll\", 'i', 'didn', 'then', 'most', 'than', 'its', 'this', 'needn', 'by', 'we', 'd', 'from', 'just', 'each', \"you're\", 'who', 'that', 'where', 'weren', 'into', 'yourselves', 'she', 'be', 'you', 'isn', 'shan', 'were', 'there', 'yourself', 'my', 'during', 'while', 'until', 'are', 'her', 'hasn', 'a', 'all', 'ours', 'not', 'whom', 'do', \"shan't\", 'can', 'yours', \"hasn't\", 'which', 'me', 'them', 'should', 'am', \"aren't\", 'between', 'some', 'wasn', 'same', \"wouldn't\", 'about', 'before', 'over', 've', 'up', 'doesn', 'did', 'those', 're', \"mustn't\", 'why', 'at', 'once', 'is', 'in', 'his', \"should've\", \"needn't\", 'themselves', 'myself', 'too', 'm', 'they', 'couldn', 'won', \"you'll\", 'if', 'doing', 'had', 'few', 'haven', 'other', 'against', 'under', 'ourselves', 'have', 'because', 'been', 'when', 'for', 'nor', 'don', 'mightn', 'being', 'aren'}\n"
     ]
    }
   ],
   "source": [
    "#making english stopwords into a set\n",
    "\n",
    "stop_words = set(stopwords.words(\"English\"))\n",
    "\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# defining a function to remove stopwords\n",
    "\n",
    "def noiseless_text(input_text):\n",
    "    \n",
    "    words = word_tokenize(input_text)                     #splitting sentence into words\n",
    "    noiseless_text =[]                                    # making a empty list\n",
    "    \n",
    "    for w in words:                                       # FOR loop to remove words from above list\n",
    "         if w not in stop_words:\n",
    "                noiseless_text.append(w)                  # appending remaining words into list\n",
    "    return print(noiseless_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['India', 'fought', 'second', 'world', 'war', '.', 'India', 'sent', 'soliders', 'supplies', 'war']\n"
     ]
    }
   ],
   "source": [
    "noiseless_text(\"India fought during second world war. India sent soliders and supplies into war\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 2 : Noise removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#making a list of meaningless words\n",
    "\n",
    "noise_text = [\"is\",\"a\",\"for\", \"that\", \"this\" , \"it\" , \"of\" , \"to\"]\n",
    "\n",
    "# creating a function to remove noise\n",
    "\n",
    "def remove_noise(input_text):\n",
    "    \n",
    "    words = input_text.split()                                                #splitting the sentence into words\n",
    "    noise_free_words = [w for w in words if w not in noise_text]              # FOR loop to remove words in above list\n",
    "    noise_free_text = \" \".join(noise_free_words)                              # joining those words\n",
    "    return noise_free_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cricket bat. give bat Virat Kohli'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_noise(\" this is a cricket bat. give this bat to Virat Kohli\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexicon normalization\n",
    "\n",
    "Another type of textual noise is about the multiple representations exhibited by single word.\n",
    "\n",
    "Example :  write, wrote, writing, writer, written \n",
    "\n",
    "The most common lexicon normalization practices are :\n",
    "\n",
    "#### Stemming:  \n",
    "\n",
    "Stemming is a rudimentary rule-based process of stripping the suffixes (“ing”, “ly”, “es”, “s” etc) from a word.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['automation', 'automatic', 'automated', 'automotive']\n"
     ]
    }
   ],
   "source": [
    "# lower casing the words\n",
    "\n",
    "example = \"Automation automatic automated automotive\"\n",
    "\n",
    "example_lower = example.lower().split()\n",
    "print(example_lower)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#stemming\n",
    "#import stemmer\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "autom\n",
      "automat\n",
      "autom\n",
      "automot\n"
     ]
    }
   ],
   "source": [
    "#FOR loop for using Porter Stemmer\n",
    "\n",
    "for word in example_lower:\n",
    "    stemmed_word = ps.stem(word)\n",
    "    print(stemmed_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Lemmatization:\n",
    "\n",
    "Lemmatization, on the other hand, is an organized & step by step procedure of obtaining the root form of the word, it makes use of vocabulary (dictionary importance of words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import lemmatizer \n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "lem = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "example2 = [\"maker\", \"called\" , \"ears\", \"loving\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make\n",
      "called\n",
      "ear\n",
      "loving\n"
     ]
    }
   ],
   "source": [
    "#FOR loop for using lemmatizer\n",
    "\n",
    "for word in example2:\n",
    "    lemmatized_word =  lem.lemmatize(word)\n",
    "    print(lemmatized_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fast\n",
      "fast\n",
      "fast\n"
     ]
    }
   ],
   "source": [
    "#lemmatizer with POS as 'adjective' example\n",
    "\n",
    "ex_3 =  [\"fast\", \"faster\",\"fastest\"]\n",
    "for word in ex_3:\n",
    "    lemmatized_word =  lem.lemmatize(word, pos = 'a')\n",
    "    print(lemmatized_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Object Standardization:\n",
    "\n",
    "Text data often contains words or phrases which are not present in any standard dictionaries\n",
    "\n",
    "Ex: \n",
    "    'awsm' means awesome.\n",
    "    'omg' means 'oh my god'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# making a replacement dictionary\n",
    "\n",
    "replace_dict = {'awsm':'awesome', 'luv': 'love', 'thnq':'Thank you'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# writing a function to replace words with new words\n",
    "\n",
    "def replace_words(input_text):\n",
    "    words = input_text.split()                  #splitting the text into words\n",
    "    new_words =[]                               #making empty list for new words\n",
    "    for w in words:                             # FOR loop for looking words in replace dict\n",
    "        if w.lower() in replace_dict:   \n",
    "            w = replace_dict[w.lower()]         #replacing them with new words\n",
    "        new_words.append(w)                     # appending words into new words list\n",
    "        new_text = ' '.join(new_words)          # joining words into a sentence again\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is awsm. I love it . Thank you .'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#example for object standarization:\n",
    "\n",
    "replace_words(\"This is awsm. I luv it . Thnq .\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering on text data\n",
    "\n",
    "### Syntactic Parsing\n",
    "\n",
    "Syntactical parsing invol ves the analysis of words in the sentence for grammar and their arrangement in a manner that shows the relationships among the words.\n",
    "\n",
    "1. Dependency trees\n",
    "2. Parts of speech tagging\n",
    "\n",
    "#### Parts of speech tagging (POS tags):\n",
    "\n",
    "Every word in a sentence is associated with a part of speech.\n",
    "\n",
    "The pos tags defines the usage and function of a word in the sentence.\n",
    "\n",
    "https://pythonprogramming.net/natural-language-toolkit-nltk-part-speech-tagging/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#importing POS tags:\n",
    "\n",
    "from nltk import pos_tag, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Cheetah', 'NNP'), ('is', 'VBZ'), ('fastest', 'JJS'), ('animal', 'NN'), ('on', 'IN'), ('earth', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "# example for POS tags\n",
    "\n",
    "pos_example = \"Cheetah is fastest animal on earth\"\n",
    "pos_words = word_tokenize(pos_example)\n",
    "\n",
    "print (pos_tag(pos_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "### Entity Extraction/ Entity parsing\n",
    "\n",
    "Entities are defined as the most important chunks of a sentence – noun phrases, verb phrases or both.\n",
    "\n",
    "Three Methods: \n",
    "\n",
    "A. Named Entity recognition\n",
    "B. Topic modelling\n",
    "C. N-grams as features\n",
    "\n",
    "#### Named entity recognition\n",
    "\n",
    "The process of detecting the named entities such as person names, location names, company names etc from the text is called as Named Entity Recognition.\n",
    "\n",
    "ex: Jack works as manager in Apple.inc in New York\n",
    "\n",
    "\n",
    "name: Jack ,  org: Apple , place: New York\n",
    "\n",
    "\n",
    "#### Topic modeling \n",
    "\n",
    "Topic modeling is a process of automatically identifying the topics present in a text \n",
    "\n",
    "Topics are defined as “a repeating pattern of co-occurring terms in a text\n",
    "\n",
    "Latent Dirichlet Allocation (LDA) is the most popular topic modelling technique\n",
    "\n",
    "#### N- grams as feature \n",
    "\n",
    "A combination of N words together are called N-Grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
